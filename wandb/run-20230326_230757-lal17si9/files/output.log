
  0%|                             | 0/72440 [00:00<?, ?it/s]/local/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '













  0%|                 | 102/72440 [00:31<4:56:46,  4.06it/s]












  0%|                 | 199/72440 [00:55<4:59:43,  4.02it/s]












  0%|                 | 297/72440 [01:19<4:53:11,  4.10it/s]













  1%|                 | 401/72440 [01:45<4:59:14,  4.01it/s]












  1%|                 | 496/72440 [02:09<5:01:24,  3.98it/s]













  1%|▏                | 600/72440 [02:35<4:48:10,  4.15it/s]













  1%|▏                | 703/72440 [03:01<4:57:23,  4.02it/s]












  1%|▏                | 799/72440 [03:25<5:02:44,  3.94it/s]













  1%|▏                | 905/72440 [03:52<5:03:16,  3.93it/s]






  1%|▏                | 956/72440 [04:04<4:54:34,  4.04it/s]Traceback (most recent call last):
  File "/local/musaeed/T5LowResourceGeneration/T5_lowresourceGeneration.py", line 45, in <module>
    trainer.train()
  File "/local/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/transformers/trainer.py", line 1633, in train
    return inner_training_loop(
  File "/local/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/transformers/trainer.py", line 1902, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/local/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/transformers/trainer.py", line 2645, in training_step
    loss = self.compute_loss(model, inputs)
  File "/local/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/transformers/trainer.py", line 2677, in compute_loss
    outputs = model(**inputs)
  File "/local/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/local/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/local/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 172, in replicate
    return replicate(module, device_ids, not torch.is_grad_enabled())
  File "/local/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/nn/parallel/replicate.py", line 115, in replicate
    replica = module._replicate_for_data_parallel()
  File "/local/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1973, in _replicate_for_data_parallel
    replica._is_replica = True  # type: ignore[assignment]
  File "/local/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1220, in __setattr__
    if isinstance(value, Parameter):
  File "/local/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/nn/parameter.py", line 11, in __instancecheck__
    isinstance(instance, torch.Tensor) and getattr(instance, '_is_param', False))
KeyboardInterrupt